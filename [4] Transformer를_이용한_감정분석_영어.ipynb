{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QcR8PpNz_DSO"},"source":["# Transformer를 이용한 감정분석기 - 영어 Data\n","\n","제작자: Park Chanjun (박찬준) <br>\n","소속: Korea University Natural Language Processing & Artificial Intelligence Lab<br>\n","Email: bcj1210@naver.com<br>\n","참고자료: https://keras.io/\n"]},{"cell_type":"markdown","metadata":{"id":"gjvMHpSS_DSv"},"source":["## Multi head Attention\n"]},{"cell_type":"code","metadata":{"id":"bIcpQjI__DSy","executionInfo":{"status":"ok","timestamp":1700622589834,"user_tz":-540,"elapsed":5702,"user":{"displayName":"이태주","userId":"07574677841390649067"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","class MultiHeadSelfAttention(layers.Layer):\n","    def __init__(self, embed_dim, num_heads=8):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        if embed_dim % num_heads != 0:\n","            raise ValueError(\n","                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n","            )\n","        self.projection_dim = embed_dim // num_heads #Multi-head Attention에서는 query, key, value를 바로 사용하는 것이 아닌 h번의 Linear projection을 따라 서로 다른 representation의 조합으로부터 Attention을 계산하는 방법이다.\n","        self.query_dense = layers.Dense(embed_dim) #쿼리\n","        self.key_dense = layers.Dense(embed_dim) #키\n","        self.value_dense = layers.Dense(embed_dim) #밸류\n","        self.combine_heads = layers.Dense(embed_dim)\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True) #Q와 V를 곱한다.\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) #텐서를 새로운 자료형으로 변환합니다.(tf.shape(key)[-1] = 차원)\n","        scaled_score = score / tf.math.sqrt(dim_key) #Sclae 작업, K차원의 루트값으로\n","        weights = tf.nn.softmax(scaled_score, axis=-1) #Softmax\n","        output = tf.matmul(weights, value) #V 곱하기\n","        return output, weights\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) #Multihead Attention\n","        return tf.transpose(x, perm=[0, 2, 1, 3]) #x를 전치합니다. perm에 따라 차원의 순서를 구성합니다.\n","\n","    def call(self, inputs):\n","        # x.shape = [batch_size, seq_len, embedding_dim]\n","        batch_size = tf.shape(inputs)[0] #배치크기\n","        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n","        query = self.separate_heads(\n","            query, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim) => tf.transpose(x, perm=[0, 2, 1, 3])의 결과\n","        key = self.separate_heads(\n","            key, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        value = self.separate_heads(\n","            value, batch_size\n","        )  # (batch_size, num_heads, seq_len, projection_dim)\n","        attention, weights = self.attention(query, key, value) #Self Attention\n","        attention = tf.transpose(\n","            attention, perm=[0, 2, 1, 3]\n","        )  # (batch_size, seq_len, num_heads, projection_dim)\n","        concat_attention = tf.reshape(\n","            attention, (batch_size, -1, self.embed_dim)\n","        )  # (batch_size, seq_len, embed_dim)\n","        output = self.combine_heads(\n","            concat_attention\n","        )  # (batch_size, seq_len, embed_dim)\n","        return output\n","\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1jlqiEx_DTE"},"source":["## Transformer Layer\n"]},{"cell_type":"code","metadata":{"id":"mJh4zMyQ_DTH","executionInfo":{"status":"ok","timestamp":1700622595128,"user_tz":-540,"elapsed":531,"user":{"displayName":"이태주","userId":"07574677841390649067"}}},"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs) #Multihead Attn 블록\n","        attn_output = self.dropout1(attn_output, training=training) #드롭아웃\n","        out1 = self.layernorm1(inputs + attn_output) #LM + Residual\n","        ffn_output = self.ffn(out1) #FF 블록\n","        ffn_output = self.dropout2(ffn_output, training=training) #드롭아웃\n","        return self.layernorm2(out1 + ffn_output) #LM + Residual\n","\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Do6Cllg7_DTY"},"source":["## 임베딩 Layer\n"]},{"cell_type":"code","metadata":{"id":"bfIfxHp4_DTb","executionInfo":{"status":"ok","timestamp":1700622686930,"user_tz":-540,"elapsed":587,"user":{"displayName":"이태주","userId":"07574677841390649067"}}},"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, emded_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=emded_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=emded_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1) #포지션 정보\n","        positions = self.pos_emb(positions) #포지션 임베딩\n","        x = self.token_emb(x) #토큰임베딩\n","        return x + positions #합치기"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sru5nYdR_DTp"},"source":["## 데이터셋 준비 및 전처리\n"]},{"cell_type":"code","metadata":{"id":"kAU_Fpdr_DTr","outputId":"87da407e-92c9-479d-bf47-61be2220defa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700623423151,"user_tz":-540,"elapsed":7270,"user":{"displayName":"이태주","userId":"07574677841390649067"}}},"source":["vocab_size = 20000  # Only consider the top 20k words\n","maxlen = 200  # Only consider the first 200 words of each movie review\n","(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size) #데이터셋 로딩\n","\n","print(len(x_train), \"Training sequences\")\n","print(len(x_val), \"Validation sequences\")\n","\n","#x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)#패딩\n","#x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)#패딩"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["25000 Training sequences\n","25000 Validation sequences\n"]}]},{"cell_type":"code","source":["x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)#패딩\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)#패딩"],"metadata":{"id":"X7NxjsdtAwC4","executionInfo":{"status":"ok","timestamp":1700623425169,"user_tz":-540,"elapsed":625,"user":{"displayName":"이태주","userId":"07574677841390649067"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bSmZHtzz_DT9"},"source":["## 모델 구축\n"]},{"cell_type":"code","metadata":{"id":"PnJPcX-5_DUB"},"source":["embed_dim = 32  # Embedding size for each token\n","num_heads = 2  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","\n","inputs = layers.Input(shape=(maxlen,)) #처음 입력\n","embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim) #객체 생성\n","x = embedding_layer(inputs)  #포지셔널 임베딩\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim) #객체 생성\n","x = transformer_block(x) #트랜스포머\n","x = layers.GlobalAveragePooling1D()(x) #Average Pooling\n","x = layers.Dropout(0.1)(x) #드롯아웃\n","x = layers.Dense(20, activation=\"relu\")(x) #FFNN\n","x = layers.Dropout(0.1)(x) #드롭아웃\n","outputs = layers.Dense(2, activation=\"softmax\")(x) #Softmax\n","\n","model = keras.Model(inputs=inputs, outputs=outputs) #모델 생성"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R7knaKVr_DUU"},"source":["## 학습\n"]},{"cell_type":"code","metadata":{"id":"kJM7h8Ax_DUX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700540288803,"user_tz":-540,"elapsed":510095,"user":{"displayName":"이태주","userId":"07574677841390649067"}},"outputId":"8b14c8f4-b337-48c0-db25-b38910b57d77"},"source":["model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val)\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","782/782 [==============================] - 94s 110ms/step - loss: 0.3864 - accuracy: 0.8152 - val_loss: 0.3013 - val_accuracy: 0.8696\n","Epoch 2/5\n","782/782 [==============================] - 85s 108ms/step - loss: 0.2032 - accuracy: 0.9228 - val_loss: 0.3280 - val_accuracy: 0.8672\n","Epoch 3/5\n","782/782 [==============================] - 104s 134ms/step - loss: 0.1379 - accuracy: 0.9491 - val_loss: 0.3438 - val_accuracy: 0.8625\n","Epoch 4/5\n","782/782 [==============================] - 87s 112ms/step - loss: 0.0927 - accuracy: 0.9693 - val_loss: 0.4951 - val_accuracy: 0.8537\n","Epoch 5/5\n","782/782 [==============================] - 86s 110ms/step - loss: 0.0613 - accuracy: 0.9794 - val_loss: 0.6001 - val_accuracy: 0.8452\n"]}]},{"cell_type":"markdown","metadata":{"id":"mGOxciDWhsTD"},"source":["## 성능측정\n"]},{"cell_type":"code","metadata":{"id":"amTGEUGHAJyl","outputId":"3875dce1-f5eb-42bf-919b-ebb447b82702","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700540311962,"user_tz":-540,"elapsed":23202,"user":{"displayName":"이태주","userId":"07574677841390649067"}}},"source":["#모델 정보 출력\n","model.summary()\n","\n","#성능 측정\n","test_loss,test_acc=model.evaluate(x_val,y_val)\n","print(\"Test_acc: \",test_acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 200)]             0         \n","                                                                 \n"," token_and_position_embeddi  (None, 200, 32)           646400    \n"," ng (TokenAndPositionEmbedd                                      \n"," ing)                                                            \n","                                                                 \n"," transformer_block (Transfo  (None, 200, 32)           6464      \n"," rmerBlock)                                                      \n","                                                                 \n"," global_average_pooling1d (  (None, 32)                0         \n"," GlobalAveragePooling1D)                                         \n","                                                                 \n"," dropout_2 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_6 (Dense)             (None, 20)                660       \n","                                                                 \n"," dropout_3 (Dropout)         (None, 20)                0         \n","                                                                 \n"," dense_7 (Dense)             (None, 2)                 42        \n","                                                                 \n","=================================================================\n","Total params: 653566 (2.49 MB)\n","Trainable params: 653566 (2.49 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","782/782 [==============================] - 23s 29ms/step - loss: 0.6001 - accuracy: 0.8452\n","Test_acc:  0.8452000021934509\n"]}]}]}